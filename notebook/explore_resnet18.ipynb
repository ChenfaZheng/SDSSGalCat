{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: C:\\Users\\zheng\\Documents\\GitHub\\SDSSGalCat\n",
      "Is mps avaliable? : False\n",
      "Is cuda avaliable? : True\n",
      "cuda device count: 1\n",
      "cuda current device: 0\n",
      "cuda device name: NVIDIA GeForce RTX 4060 Ti\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "BASE = Path().resolve().parent\n",
    "print(f'BASE: {BASE}')\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "IS_MPS_AVAILABLE = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "print(f'Is mps avaliable? : {IS_MPS_AVAILABLE}')\n",
    "if not IS_MPS_AVAILABLE:\n",
    "    IS_CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "    print(f'Is cuda avaliable? : {IS_CUDA_AVAILABLE}')\n",
    "    print(f'cuda device count: {torch.cuda.device_count()}')\n",
    "    print(f'cuda current device: {torch.cuda.current_device()}')\n",
    "    print(f'cuda device name: {torch.cuda.get_device_name()}')\n",
    "    # switch to cuda if available, else mps, else cpu\n",
    "    DEVICE = torch.device('cuda' if IS_CUDA_AVAILABLE else 'cpu')\n",
    "    print(f'device: {DEVICE}')\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\zheng/.cache\\torch\\hub\\v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\zheng/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 9.91MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try resnet18\n",
    "# ref: https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: (21785, 69, 69, 3), labels.shape: (21785,)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# data ref: https://astronn.readthedocs.io/en/stable/galaxy10sdss.html\n",
    "\n",
    "with h5py.File(BASE / 'data/Galaxy10.h5', 'r') as f:\n",
    "    images = f['images'][:]\n",
    "    labels = f['ans'][:]\n",
    "\n",
    "print(f'images.shape: {images.shape}, labels.shape: {labels.shape}')\n",
    "assert images.shape[0] == labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "img = Image.fromarray(images[0], mode='RGB')\n",
    "input_tensor = preprocess(img)\n",
    "print(input_tensor.shape)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "print(input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9724e-03, 1.6914e-03, 6.3476e-05, 8.6612e-05, 1.1132e-04, 4.7789e-03,\n",
      "        1.4083e-05, 1.0921e-05, 5.7664e-05, 1.6972e-04, 2.1852e-04, 1.4466e-05,\n",
      "        5.3572e-06, 7.1628e-06, 6.4568e-06, 1.3169e-05, 3.3717e-06, 3.1598e-06,\n",
      "        1.5244e-05, 2.3823e-06, 4.6630e-05, 1.5864e-05, 1.5739e-04, 8.5525e-05,\n",
      "        9.1811e-05, 3.5262e-05, 1.4006e-04, 5.1545e-05, 2.2844e-04, 2.5061e-03,\n",
      "        2.1433e-05, 9.4596e-05, 1.8174e-04, 3.8943e-04, 8.6953e-04, 1.2722e-04,\n",
      "        1.4212e-04, 1.3122e-05, 2.5656e-03, 1.7913e-04, 4.0059e-05, 1.4075e-04,\n",
      "        8.6553e-05, 4.2352e-03, 1.4009e-04, 5.7675e-04, 2.0879e-05, 2.7877e-04,\n",
      "        1.3059e-04, 7.4135e-05, 6.1240e-05, 1.1688e-03, 4.4015e-05, 2.8316e-05,\n",
      "        2.5375e-05, 2.3510e-05, 2.6387e-05, 4.8829e-06, 1.3947e-05, 1.1758e-03,\n",
      "        2.2857e-04, 1.8477e-05, 6.4050e-04, 9.2615e-04, 3.8121e-04, 1.2277e-03,\n",
      "        4.5590e-04, 1.8334e-05, 5.3920e-04, 9.9821e-06, 1.2219e-05, 2.0904e-04,\n",
      "        2.3430e-05, 5.4578e-03, 1.1523e-05, 5.8129e-05, 1.0423e-04, 2.4581e-05,\n",
      "        5.5966e-04, 1.1265e-04, 2.6965e-03, 8.6541e-05, 1.1133e-04, 4.7954e-04,\n",
      "        6.3067e-05, 2.5786e-05, 3.3914e-04, 1.5204e-05, 3.5632e-05, 6.2025e-05,\n",
      "        2.6605e-05, 5.3065e-06, 1.4086e-05, 2.5080e-05, 7.8810e-06, 3.8753e-05,\n",
      "        1.3879e-04, 5.0663e-06, 2.1884e-04, 1.3150e-04, 6.4182e-05, 5.3585e-04,\n",
      "        9.5970e-05, 5.9625e-04, 2.1291e-04, 6.8774e-04, 7.5177e-04, 6.1114e-04,\n",
      "        5.8974e-03, 8.9339e-04, 4.0376e-04, 3.3551e-03, 2.1497e-04, 5.4358e-04,\n",
      "        8.2077e-05, 2.0072e-03, 2.1929e-03, 1.1272e-01, 1.2180e-04, 6.7297e-04,\n",
      "        7.4971e-05, 4.5808e-04, 5.8712e-05, 2.0139e-04, 1.6938e-05, 1.6045e-04,\n",
      "        3.2578e-03, 2.1061e-05, 1.4530e-03, 1.6861e-04, 2.4643e-04, 5.1808e-06,\n",
      "        1.5175e-04, 1.4422e-05, 8.2917e-05, 3.7166e-04, 5.8519e-06, 3.2804e-05,\n",
      "        9.5811e-06, 8.0208e-06, 8.1615e-06, 3.1273e-06, 9.8879e-06, 1.3860e-05,\n",
      "        6.8650e-05, 9.7092e-04, 6.1377e-05, 5.6138e-04, 8.9530e-04, 2.7581e-03,\n",
      "        3.0596e-04, 4.6885e-05, 6.8427e-04, 2.8147e-04, 4.9256e-05, 5.7614e-05,\n",
      "        1.3909e-04, 6.2232e-05, 2.3903e-04, 3.5991e-05, 6.1265e-05, 9.8093e-06,\n",
      "        2.4684e-05, 1.1793e-05, 8.4905e-04, 1.9564e-04, 1.9982e-04, 4.1688e-04,\n",
      "        2.3070e-04, 1.2806e-04, 9.4750e-05, 1.0758e-04, 1.6652e-05, 2.5841e-04,\n",
      "        6.2047e-05, 4.5209e-04, 3.3558e-05, 2.0973e-04, 8.1579e-05, 3.6784e-05,\n",
      "        1.1113e-04, 8.0286e-04, 4.5579e-05, 8.8670e-03, 2.4720e-05, 4.4775e-05,\n",
      "        6.5460e-05, 7.0558e-05, 2.6402e-04, 2.6609e-04, 1.9237e-03, 1.1707e-04,\n",
      "        9.1895e-05, 4.8662e-05, 9.8321e-04, 6.0469e-05, 3.1933e-04, 7.9076e-05,\n",
      "        8.4465e-05, 1.0213e-04, 1.7425e-04, 3.8557e-05, 5.8243e-05, 2.0042e-04,\n",
      "        1.3515e-04, 1.4035e-05, 1.5288e-03, 1.4358e-04, 2.1786e-04, 9.0442e-05,\n",
      "        1.0143e-04, 2.4417e-05, 7.4341e-05, 1.9172e-05, 2.2857e-05, 1.6630e-04,\n",
      "        1.3274e-04, 1.8482e-05, 2.4067e-05, 2.2969e-05, 1.2589e-04, 7.1092e-03,\n",
      "        6.7200e-04, 1.1159e-04, 1.5174e-04, 2.6487e-05, 1.4650e-04, 3.0783e-05,\n",
      "        1.9220e-03, 5.4651e-04, 5.3610e-05, 7.6220e-05, 1.1310e-04, 7.3572e-04,\n",
      "        8.4932e-05, 2.6781e-05, 1.3070e-04, 2.9953e-05, 1.6086e-04, 1.6209e-04,\n",
      "        1.8145e-04, 8.5862e-04, 5.6528e-05, 7.0611e-05, 1.0908e-04, 1.6389e-05,\n",
      "        1.2012e-04, 6.6822e-05, 5.5784e-04, 1.4466e-04, 7.2402e-04, 5.9073e-05,\n",
      "        2.9680e-03, 6.5554e-05, 2.6933e-05, 1.3032e-05, 6.2457e-05, 3.4039e-04,\n",
      "        1.6081e-04, 3.0095e-04, 1.0221e-04, 1.3722e-04, 7.5899e-05, 1.7336e-05,\n",
      "        3.4884e-05, 3.0225e-04, 1.1017e-03, 1.1245e-03, 8.0833e-05, 6.9477e-04,\n",
      "        2.9619e-04, 1.4260e-03, 3.1227e-04, 6.9163e-04, 6.6752e-05, 3.1642e-04,\n",
      "        6.9203e-05, 4.6089e-04, 2.8314e-04, 1.2410e-03, 5.9565e-03, 3.0585e-04,\n",
      "        2.6206e-04, 5.8627e-04, 1.7215e-04, 4.1747e-04, 2.0792e-04, 9.5264e-05,\n",
      "        7.0386e-04, 4.3692e-04, 1.7390e-04, 1.4670e-04, 1.1761e-04, 2.7632e-04,\n",
      "        1.1469e-03, 3.5110e-04, 5.9704e-04, 7.8918e-03, 1.5030e-04, 2.6344e-04,\n",
      "        9.4938e-06, 9.8263e-06, 1.5956e-04, 1.4395e-05, 4.3818e-06, 2.4116e-05,\n",
      "        1.9048e-05, 1.5569e-05, 1.0154e-05, 4.0597e-06, 5.5797e-04, 4.3253e-06,\n",
      "        9.2219e-06, 1.0798e-04, 1.1466e-05, 5.5146e-05, 1.6579e-05, 5.3829e-06,\n",
      "        1.1267e-04, 5.6683e-05, 2.9700e-05, 2.9477e-06, 1.7447e-05, 2.0596e-05,\n",
      "        3.2000e-05, 1.0699e-05, 1.1263e-05, 8.5123e-04, 1.3684e-03, 7.3573e-04,\n",
      "        3.0752e-04, 2.8294e-04, 6.0720e-04, 3.0703e-05, 1.2852e-03, 6.0762e-03,\n",
      "        1.6356e-04, 3.3294e-04, 1.1879e-04, 7.1398e-04, 7.4012e-05, 3.1213e-04,\n",
      "        2.4273e-04, 4.0392e-05, 1.4426e-04, 2.6462e-04, 3.5552e-06, 5.7860e-05,\n",
      "        1.0776e-05, 1.2359e-04, 6.9153e-05, 1.3003e-04, 2.1173e-04, 2.3459e-04,\n",
      "        4.0380e-04, 7.4973e-05, 5.9459e-04, 7.1712e-04, 2.8703e-03, 1.6185e-03,\n",
      "        1.4457e-04, 3.7303e-03, 3.2473e-03, 3.4676e-04, 1.5046e-03, 8.1229e-04,\n",
      "        1.1655e-04, 1.2418e-04, 5.9429e-04, 1.2069e-03, 4.0699e-03, 9.0481e-03,\n",
      "        1.0838e-03, 3.6931e-04, 1.2009e-03, 2.9303e-03, 8.3805e-04, 6.4824e-03,\n",
      "        2.4037e-04, 5.4476e-05, 2.0736e-03, 1.7482e-04, 5.8152e-04, 1.5364e-03,\n",
      "        9.2125e-03, 3.8345e-04, 5.2407e-05, 2.1670e-03, 1.3841e-03, 4.0160e-04,\n",
      "        2.2475e-03, 4.6283e-05, 9.7122e-03, 2.3643e-03, 7.3828e-05, 3.1754e-04,\n",
      "        2.3776e-04, 2.9909e-04, 4.3509e-05, 5.8401e-04, 1.1529e-04, 5.5071e-05,\n",
      "        1.3933e-04, 2.5773e-04, 1.8539e-04, 4.1463e-04, 1.8556e-04, 1.0182e-03,\n",
      "        4.8840e-04, 6.5662e-03, 7.1989e-05, 3.5741e-06, 1.9229e-04, 1.9193e-03,\n",
      "        2.6760e-06, 6.9788e-06, 3.2614e-04, 1.7737e-03, 1.6142e-04, 9.1429e-04,\n",
      "        3.2606e-04, 9.3258e-04, 1.4621e-05, 4.0344e-05, 4.6938e-05, 1.2722e-04,\n",
      "        9.4941e-04, 4.2178e-03, 2.0227e-04, 6.3919e-03, 3.9337e-04, 1.2015e-05,\n",
      "        3.2529e-04, 3.9218e-05, 7.2797e-06, 1.0368e-04, 3.2687e-05, 2.5517e-03,\n",
      "        6.0044e-04, 3.4103e-03, 1.4099e-04, 2.5575e-04, 9.4833e-04, 1.1504e-05,\n",
      "        6.7441e-05, 1.4134e-04, 1.8661e-04, 9.5506e-04, 5.0915e-05, 2.5042e-03,\n",
      "        5.5773e-05, 4.9739e-03, 7.1931e-05, 3.5752e-05, 1.5144e-05, 7.8538e-05,\n",
      "        3.9343e-04, 3.9100e-05, 1.8493e-04, 9.7508e-04, 2.1724e-04, 3.4829e-04,\n",
      "        3.2200e-04, 1.0122e-04, 1.1528e-04, 8.9516e-04, 9.5389e-05, 3.3937e-06,\n",
      "        1.2965e-04, 3.8249e-03, 2.7195e-03, 5.4723e-04, 2.4501e-04, 7.4347e-05,\n",
      "        4.4012e-05, 3.0990e-04, 3.4493e-05, 1.5038e-05, 7.9027e-06, 5.7146e-05,\n",
      "        4.7952e-04, 1.8404e-03, 1.2221e-03, 2.1578e-03, 1.2268e-04, 6.2576e-05,\n",
      "        4.5327e-04, 2.2857e-04, 9.3573e-04, 6.6408e-04, 5.8680e-03, 1.6612e-03,\n",
      "        8.4588e-06, 2.0219e-04, 2.9732e-03, 1.3116e-05, 3.5696e-05, 1.9355e-04,\n",
      "        2.1344e-04, 7.4897e-04, 1.1203e-04, 2.0580e-03, 2.1544e-04, 1.4290e-04,\n",
      "        1.4133e-04, 5.8439e-05, 2.7554e-04, 4.7047e-04, 1.0621e-04, 5.3441e-06,\n",
      "        2.9435e-04, 3.4229e-05, 1.8027e-05, 1.1854e-04, 2.3570e-05, 2.8630e-05,\n",
      "        1.1463e-04, 2.2881e-04, 1.5300e-04, 2.2678e-04, 1.6358e-06, 2.0601e-04,\n",
      "        1.5703e-02, 1.2727e-04, 2.6445e-04, 1.6288e-03, 3.8267e-05, 3.2540e-04,\n",
      "        1.9361e-05, 6.8390e-05, 1.5463e-03, 1.1312e-03, 5.7496e-05, 9.1508e-06,\n",
      "        1.4874e-04, 1.6669e-05, 2.6511e-04, 1.4029e-04, 7.7231e-04, 1.3708e-04,\n",
      "        2.4872e-03, 5.7026e-04, 6.6631e-04, 1.0838e-04, 4.5974e-04, 4.3471e-05,\n",
      "        1.8612e-04, 2.9288e-04, 1.0160e-04, 1.0792e-05, 7.4631e-05, 4.1498e-04,\n",
      "        2.4355e-05, 1.9515e-05, 3.7675e-05, 3.3063e-04, 3.5732e-02, 6.0683e-04,\n",
      "        2.7122e-04, 7.5965e-04, 1.3544e-04, 7.2404e-05, 1.3084e-03, 2.2026e-05,\n",
      "        9.9667e-05, 1.5637e-04, 3.0804e-05, 1.4494e-03, 1.9210e-05, 2.6230e-04,\n",
      "        1.3885e-04, 1.8897e-05, 4.4328e-05, 8.0404e-05, 3.6258e-03, 1.5644e-04,\n",
      "        1.2919e-04, 2.8110e-02, 3.0440e-05, 1.9197e-05, 8.4656e-05, 4.0060e-06,\n",
      "        3.7572e-05, 2.2532e-03, 8.9140e-04, 1.4121e-03, 1.1198e-04, 7.8311e-05,\n",
      "        6.0118e-05, 4.1876e-05, 5.9317e-04, 2.2596e-05, 1.5275e-05, 3.8550e-05,\n",
      "        4.1390e-04, 9.3162e-05, 1.3090e-03, 7.4084e-06, 2.3485e-04, 1.8526e-04,\n",
      "        1.1387e-04, 6.4866e-04, 8.8405e-04, 3.7499e-05, 2.5016e-03, 3.2225e-05,\n",
      "        2.9584e-05, 1.4801e-03, 2.7143e-05, 1.8433e-04, 5.4703e-04, 9.5035e-05,\n",
      "        1.4848e-04, 2.5738e-04, 2.7985e-05, 1.1994e-03, 2.1566e-04, 8.4603e-05,\n",
      "        6.5104e-05, 4.3941e-04, 4.2723e-04, 8.1264e-05, 3.9003e-04, 5.3706e-04,\n",
      "        1.5610e-05, 5.2152e-05, 2.3784e-03, 1.6993e-04, 4.0908e-05, 3.1617e-04,\n",
      "        7.4612e-05, 1.1291e-03, 5.4038e-04, 8.4825e-03, 2.8321e-04, 4.4210e-03,\n",
      "        4.9380e-05, 3.5291e-05, 2.5825e-05, 2.2456e-04, 1.2028e-05, 1.1395e-02,\n",
      "        3.0066e-04, 7.1631e-05, 7.5816e-02, 2.5243e-04, 2.1314e-03, 1.0302e-04,\n",
      "        6.3579e-06, 1.5537e-03, 2.9134e-04, 1.4592e-03, 5.0708e-05, 4.7912e-03,\n",
      "        9.1269e-05, 1.3502e-05, 2.1717e-04, 4.3911e-03, 2.3230e-04, 4.5840e-05,\n",
      "        6.4464e-03, 6.8259e-06, 1.9306e-04, 3.2429e-03, 5.2331e-04, 1.3773e-04,\n",
      "        1.5236e-03, 3.3567e-05, 1.0018e-03, 3.5468e-05, 4.0590e-05, 2.1841e-04,\n",
      "        6.7108e-03, 1.0705e-04, 2.0890e-04, 1.5767e-03, 1.6873e-04, 1.6599e-04,\n",
      "        1.7179e-03, 1.4984e-05, 7.6868e-04, 4.8624e-05, 2.2197e-04, 7.9708e-04,\n",
      "        5.6638e-04, 6.3551e-04, 1.9476e-04, 2.1356e-05, 5.1271e-04, 5.0401e-04,\n",
      "        4.8242e-05, 1.0295e-04, 1.9774e-04, 4.1237e-04, 7.6448e-04, 7.5011e-05,\n",
      "        5.3602e-05, 6.5371e-06, 4.5840e-04, 2.0730e-03, 5.3665e-05, 7.3332e-05,\n",
      "        2.2845e-03, 3.7238e-04, 5.6909e-05, 2.6409e-04, 2.7426e-04, 5.2758e-05,\n",
      "        8.8644e-04, 1.4062e-06, 1.5257e-05, 1.6355e-04, 7.3672e-03, 2.2313e-05,\n",
      "        3.5981e-03, 4.8794e-04, 2.7815e-04, 4.1493e-05, 1.6514e-03, 2.6002e-05,\n",
      "        5.3873e-04, 5.1695e-06, 6.8773e-03, 1.6679e-03, 5.7649e-04, 8.4939e-05,\n",
      "        3.1093e-05, 1.9212e-02, 8.2821e-04, 2.0762e-04, 6.5185e-05, 1.1397e-03,\n",
      "        8.4519e-05, 1.5727e-03, 2.0821e-04, 3.7951e-05, 2.8417e-03, 5.0628e-05,\n",
      "        4.3764e-04, 1.4355e-04, 6.3413e-05, 5.9786e-04, 1.4668e-05, 1.3379e-03,\n",
      "        1.4675e-02, 7.7185e-04, 2.6227e-04, 1.4881e-03, 1.2889e-05, 3.9703e-04,\n",
      "        4.6309e-05, 3.2462e-05, 2.3754e-05, 8.4507e-05, 2.8897e-05, 5.8703e-04,\n",
      "        1.7148e-04, 4.0977e-04, 3.8516e-04, 7.7618e-04, 3.0084e-05, 3.0941e-04,\n",
      "        7.9024e-05, 6.8014e-04, 4.7894e-05, 9.0629e-05, 3.7021e-04, 1.2528e-04,\n",
      "        1.2096e-04, 6.0726e-05, 1.9538e-05, 7.9116e-04, 1.7743e-03, 6.4540e-05,\n",
      "        2.2406e-05, 6.3303e-05, 2.7655e-04, 3.4397e-05, 5.8576e-06, 9.7241e-05,\n",
      "        1.3281e-04, 1.8722e-03, 1.2905e-02, 1.6905e-04, 1.7708e-05, 9.5805e-05,\n",
      "        1.0973e-05, 1.1143e-04, 1.4600e-06, 7.9166e-04, 9.3767e-06, 3.5790e-05,\n",
      "        1.4604e-04, 3.8891e-05, 8.1479e-06, 2.5168e-04, 5.5151e-04, 5.7269e-05,\n",
      "        2.8157e-04, 3.1689e-05, 5.9169e-05, 4.4504e-05, 8.0323e-05, 9.3770e-04,\n",
      "        2.4319e-04, 1.1893e-03, 7.6011e-05, 4.5939e-04, 3.6714e-05, 1.7142e-05,\n",
      "        1.9561e-04, 5.5313e-04, 8.0500e-03, 5.2550e-04, 7.1030e-04, 1.1962e-04,\n",
      "        7.5115e-06, 5.0478e-05, 3.2623e-02, 7.1059e-04, 2.3426e-04, 1.0870e-02,\n",
      "        1.6285e-04, 5.5214e-05, 1.0890e-04, 7.1910e-04, 1.1446e-03, 1.4419e-03,\n",
      "        4.2636e-05, 2.7641e-05, 9.1697e-05, 1.2953e-04, 3.0448e-04, 1.2246e-04,\n",
      "        7.8688e-05, 9.5768e-05, 1.2997e-03, 7.3081e-05, 7.6299e-04, 1.3246e-03,\n",
      "        8.3055e-06, 4.5431e-04, 9.1381e-05, 2.2031e-04, 3.6719e-04, 1.1303e-04,\n",
      "        1.4243e-03, 6.4796e-04, 1.1999e-04, 8.5328e-06, 3.4851e-05, 2.2450e-03,\n",
      "        1.9194e-03, 5.1511e-04, 3.8608e-03, 8.9484e-05, 5.2338e-04, 3.0111e-04,\n",
      "        2.4168e-05, 1.6772e-05, 5.0887e-04, 7.1290e-05, 2.5105e-02, 4.3199e-05,\n",
      "        4.5025e-04, 2.1847e-05, 4.0349e-04, 1.5024e-03, 3.6860e-06, 3.3647e-04,\n",
      "        6.5601e-05, 3.8965e-04, 4.1843e-03, 4.3642e-04, 3.3465e-05, 1.2005e-04,\n",
      "        6.0287e-05, 1.2250e-04, 1.3362e-04, 2.1126e-04, 1.7962e-04, 2.7459e-04,\n",
      "        5.6749e-06, 2.2051e-04, 8.0314e-04, 7.0195e-04, 2.2011e-05, 1.1401e-04,\n",
      "        2.2119e-03, 2.4555e-05, 2.2869e-04, 3.4232e-05, 5.5653e-04, 5.3416e-05,\n",
      "        5.0170e-06, 7.9834e-06, 1.8025e-05, 3.6593e-05, 2.0105e-05, 1.0288e-04,\n",
      "        2.8685e-03, 3.4638e-03, 4.8396e-04, 2.7694e-04, 1.0212e-03, 1.7840e-04,\n",
      "        1.0811e-03, 9.2414e-05, 5.2355e-05, 6.4952e-04, 3.3674e-05, 6.2077e-05,\n",
      "        4.2798e-03, 6.8617e-04, 4.0049e-04, 9.5101e-05, 4.1863e-04, 2.3340e-04,\n",
      "        3.8120e-05, 6.1964e-04, 2.3985e-03, 1.7160e-03, 7.3362e-06, 1.0078e-04,\n",
      "        1.2128e-05, 5.6891e-04, 3.1096e-04, 7.4701e-05, 1.7586e-06, 4.8823e-05,\n",
      "        8.1210e-05, 3.0401e-05, 2.5276e-05, 1.2264e-05, 5.6861e-06, 8.9687e-06,\n",
      "        2.3632e-04, 4.9569e-04, 3.3716e-05, 5.6943e-05, 1.5831e-03, 1.5902e-05,\n",
      "        8.9495e-05, 2.4074e-05, 5.9389e-05, 5.5777e-05, 6.3526e-04, 1.7316e-04,\n",
      "        9.4259e-04, 2.3840e-04, 4.5996e-05, 7.9102e-05, 1.7118e-03, 4.6273e-05,\n",
      "        3.2313e-05, 2.2201e-04, 7.1534e-05, 2.6516e-05, 1.6523e-04, 3.8779e-05,\n",
      "        8.2725e-05, 7.7507e-06, 4.6823e-04, 5.4777e-06, 3.2031e-05, 4.0084e-06,\n",
      "        1.2827e-04, 4.8243e-05, 5.9580e-05, 1.5668e-03, 3.5454e-04, 5.3792e-04,\n",
      "        5.7013e-04, 5.0971e-04, 1.4235e-03, 5.7330e-03, 7.3052e-04, 2.1875e-04,\n",
      "        2.3016e-03, 1.3232e-03, 1.6797e-02, 1.4242e-04, 4.1662e-05, 3.5810e-04,\n",
      "        1.2722e-05, 1.3536e-03, 1.0147e-03, 5.6223e-04, 1.6610e-03, 2.1793e-04,\n",
      "        1.1202e-03, 2.2620e-04, 1.9171e-04, 1.2252e-04, 8.4210e-05, 2.0883e-03,\n",
      "        1.1628e-03, 2.8013e-04, 6.5810e-04, 5.7175e-06])\n"
     ]
    }
   ],
   "source": [
    "# try to run the model\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000]) torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# try on larger batch\n",
    "img_lst = [Image.fromarray(images[i], mode='RGB') for i in range(10)]\n",
    "input_tensor_lst = [preprocess(img) for img in img_lst]\n",
    "input_batch = torch.cat([tsr.unsqueeze(0) for tsr in input_tensor_lst])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "print(probabilities.shape, input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
